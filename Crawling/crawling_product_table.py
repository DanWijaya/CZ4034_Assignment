# -*- coding: utf-8 -*-
"""crawling_v2_runall.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zsXZv40dsLJ33QEH2pBaETsj4oD_IbPH
"""

import pandas as pd
import requests
from requests_html import HTMLSession
import datetime
import time

search_query="ultrawide+monitor"

base_url="https://www.amazon.com/s?k="

header={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}
proxies = {
 "http": "159.203.61.169:1080",
 "https": "159.203.61.169:1080",
}
delay = 1

cookie={} # insert request cookies within{}
def getAmazonSearch(search_query, page_number):
    url="https://www.amazon.com/s?k="+search_query+"&page=" + str(page_number)
    time.sleep(delay)
    s = HTMLSession()
    page = s.get(url)
    page.html.render(sleep=delay)
    # page=requests.get(url,headers=header, proxies=proxies)

    if page.status_code==200:
        return page
    else:
        return "Error"

def Searchasin(asin):
    url="https://www.amazon.com/dp/"+asin
    time.sleep(delay)
    s = HTMLSession()
    page = s.get(url)
    if page.status_code==200:
        return page
    else:
        return "Error"

def Searchreviews(review_link):
    url="https://www.amazon.com"+review_link
    print(url)
    time.sleep(delay)
    s = HTMLSession()
    page = s.get(url)
    if page.status_code==200:
        return page
    else:
        return "Error"


def getInfoFromAmazon(generic_product, data_asin):
    link = []
    result_list = []
    """ADDED PART"""
    for i in range(len(data_asin)):
        print("%s product retrieved..." % str(i+1))
        s = HTMLSession()
        r = s.get("https://www.amazon.com/dp/" + data_asin[i])
        r.html.render(sleep=delay)

        if(r.html.xpath("//span[@id='priceblock_ourprice']/text()", first=True)):
            price = r.html.xpath("//span[@id='priceblock_ourprice']/text()", first=True)
        elif(r.html.xpath("//*[@id='priceblock_dealprice']/text()", first=True)):
            price  = r.html.xpath("//*[@id='priceblock_dealprice']/text()", first=True)
        else:
            price = -1

        try:
            title = r.html.xpath("//*[@id='productTitle']", first=True).text
        except:
            title = ""

        try:
            image_url = r.html.xpath("//*[@id='landingImage']/@src", first=True)
        except:
            image_url = ""

        try:
            global_rating = r.html.xpath("//span[@id='acrCustomerReviewText']/text()", first=True).split()[0]
        except:
            global_rating = -1

        try:
            avg_rating = float(
                r.html.xpath('//*[@id="acrPopover"]/span[1]/a/i[1]/span', first=True).text.split()[0])
        except:
            avg_rating = -1.0

        row = (data_asin[i], generic_product,title , price,global_rating, avg_rating, image_url)

        result_list.append(row)

    return result_list

generic_products = ["Smartphone", "Television", "Headphones", "monitor", "keyboard", "router", "laptop", "Printer", "Data Storage", "Speaker"]
# generic_products = ["Laptop", "Printer", "Data Storage", "Speaker"]
final = []
info = []
for search_query in generic_products:
    q = search_query.replace(" ", "+")
    data_asin = []
    print("Getting for product: %s " % (search_query))
    for i in range(1,3):
        response=getAmazonSearch(q,i)
        data_asin.extend(response.html.xpath("//*[@class='s-result-item s-asin sg-col-0-of-12 sg-col-16-of-20 sg-col sg-col-12-of-16']/@data-asin"))

    info.extend(getInfoFromAmazon(search_query, data_asin))

df = pd.DataFrame(info, columns=['product_id', 'generic_product','product','price','num_of_ratings','avg_rating','image'])
df.to_csv("product_lists.csv")


